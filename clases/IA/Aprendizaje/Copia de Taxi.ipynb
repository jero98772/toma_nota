{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dm5n2EPzAGQN4QgZUhlqBfJ-EfqcMNzG","timestamp":1603708164363}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"--buM_MHAHQS"},"source":["Aprendizaje Reforzado"]},{"cell_type":"markdown","metadata":{"id":"x3PLfI-m6SS8"},"source":["**Aprendizaje Reforzado**\n","\n","> En este ejercicio se tiene una simulación de la problemática abordada por un taxi común, se basa principalmente en los movimientos que debe realizar este para recoger y transportar un pasajero a su lugar de destino. Como en un entorno real, existirán movimientos prohibidos, rutas más largas y rutas más cortas de un punto a otro, a través de las cuales el taxi deberá llevar al pasajero, obteniendo recompensas de acuerdo a la eficiencia y los movimientos realizados dentro del espacio determinado.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vTpbSoyF8J3K"},"source":["Se valida que se tenga la libreria instalada"]},{"cell_type":"code","metadata":{"id":"4BHGzLFNAY8_","executionInfo":{"status":"ok","timestamp":1681494300724,"user_tz":300,"elapsed":7,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["#!pip install cmake 'gym[atari]' scipy"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1bWNDAS9E2_"},"source":["Se importa la libreria gym la cual posee varios escenarios y problematicas que pueden aplicarse a traves de aprendizaje reforzado y se llama el ambiente del problema que se va a trabajar, en este caso el entorno del taxi. Posteriormente se grafica el estado inicial de este."]},{"cell_type":"code","metadata":{"id":"gvtVSSD--cuq","colab":{"base_uri":"https://localhost:8080/","height":541},"executionInfo":{"status":"error","timestamp":1681494301139,"user_tz":300,"elapsed":421,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}},"outputId":"9b416a6d-a8bd-4071-aa05-c92a7c48349e"},"source":["import gym\n","\n","taxi = gym.make(\"Taxi-v3\").env\n","\n","taxi.render()"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.9/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n","If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n"]},{"output_type":"error","ename":"ResetNeeded","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResetNeeded\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b3e064cdfcea>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taxi-v3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtaxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;34m\"\"\"Renders the environment with `kwargs`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_render_order_enforcing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             raise ResetNeeded(\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0;34m\"Cannot call `env.render()` before calling `env.reset()`, if this is a intended action, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34m\"set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResetNeeded\u001b[0m: Cannot call `env.render()` before calling `env.reset()`, if this is a intended action, set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper."]}]},{"cell_type":"markdown","metadata":{"id":"YUuvjVr2t2hd"},"source":[" \n","*   El cuadro amarillo representa el taxi, el cual es amarillo cuando está vacio y verde cuando lleva un pasajero.\n","*   El simbolo pipe (|) representa paredes que el taxi no puede cruzar.\n","*   R, G, Y, B son las posibles localizaciones donde se puede recoger y dejar un pasajero. La letra azul representa el actual lugar de recoger el pasajero. La letra morada será el destino.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hc195gaL9yvg"},"source":["***Problema Markoviano***\n","\n","\n","*   El agente sería nuestro vehículo\n","*  Los estados son cada uno de los lugares en donde el taxi puede ir, además de las únicas posiciones en donde puede recoger o dejar el pasajero. En total, son 500 posibles estados que vienen dados por: la grilla de la ciudad de 5x5 (25 posibles localizaciones del taxi), a esto se  multiplica los 4 lugares de destino (R, G, Y, B o en coordenadas: [(0,0), (0,4), (4,0),(4,3)) teniendo 5x5x4, y finalmente los 4 lugares de recogida del pasajero (que son los mismos puntos R, G, Y, B) al cual hay que sumarle la posibilidad de que esté el pasajero en el carro (4+1), teniendo entonces 5x5x4x5. Eso nos da los 500 posibles estados en los que el taxi se puede encontrar.\n","\n","\n","* El conjunto de acciones que puede ejecutar el agente serían 6:\n","> 1. Ir al norte\n","> 2. Ir al sur\n","> 3. Ir al este\n","> 4. Ir al oeste\n","> 5. Recoger al pasajero\n","> 6. Dejar al pasajero\n","\n","* La probabilidad de transición es igual para todas las transiciones entre estados, pues en el escenario evaluado el vehículo puede dirigirse a cualquier dirección sin ningún tipo de sesgo.\n","\n","* Como el \"agente\" va a aprender a manejar el taxi experimentando en el ambiente, las recompensas y las penalidades a considerar son:\n","> * El agente debe recibir una recompensa positiva alta llevar al pasajero a su destino de manera exitosa, porque este comportamiento es muy deseado (+20 por dejar de manera exitosa al pasajero).\n","> * El agente debe ser penalizado si lleva o recoge al pasajero a un lugar incorrecto (-10 por dejar o recoger al pasajero en lugares no permitidos).\n","> * El agente debe tener una recompensa levemente negativa por no llegar a la localización en cada paso (-1 por cada paso). \n","> * El agente cuando realiza una accion que no se puede realizar en ciertos estados, porque hay \"paredes\", se hará una penalización de -1 cada que se choque con una pared.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mxgN1DG69o_M"},"source":["Posteriormente se hace un reinicio del estado inicial del taxi para que este se ubique de manera aleatoria en otra posicion y estado."]},{"cell_type":"code","metadata":{"id":"kki46Fjp-qFW","executionInfo":{"status":"aborted","timestamp":1681494301141,"user_tz":300,"elapsed":19,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["taxi.reset() # se reinicia el ambiente a un nuevo estado de manera aleatoria\n","taxi.render()\n","\n","print(\"Espacio de acciones {}\".format(taxi.action_space))\n","print(\"Espacio de estados {}\".format(taxi.observation_space))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jkO-mRlK__mJ"},"source":["Necesitamos una manera de identificar cada estado de una manera unica, asignando un unico numero a cada posible estado y el algoritmo de aprendizaje reforzado aprenderá a tomar una acción con un número de 0 a 5, donde: \n","\n","**A. Acciones:**\n","0 = south;\n","1 = north;\n","2 = east;\n","3 = west;\n","4 = pickup;\n","5 = dropoff;\n","\n","**B. Localización del pasajero:**\n","0: R;\n","1: G;\n","2: Y;\n","3: B;\n","4: en el taxi\n","\n","**C. Destino:**\n","0: R;\n","1: G;\n","2: Y;\n","3: B\n","\n","\n","Por ejemplo, supongamos que:\n","\n","* El taxi está en la posición  (3,1) \n","* El pasajero está en la localización Y, que es  2 \n","* El destino es R, que es  0 \n","\n","*Nuestro estado será: (3,1,2,0)*\n","---\n","\n","\n","Con el método taxi.encode podemos codificar el estado. Miremos:"]},{"cell_type":"code","metadata":{"id":"0_qJgvD0yqWz","executionInfo":{"status":"aborted","timestamp":1681494301142,"user_tz":300,"elapsed":20,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["estado = taxi.encode(3,1,2,0) # fila taxi, columna taxi, localización pasajero, destino\n","print(\"Estado: \", estado)\n","\n","taxi.s = estado\n","taxi.render()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IqJ5aOQfyrNN"},"source":["Estamos usando las coordenadas de la imagen para generar un número de  0−499  correspondiente a un estado, en este caso es el estado  328 .\n","\n","Así se puede hacer para todas las posibles combinaciones y cada una tendrá un valor de  0−499  y se podrá hacer de manera manual como ya se vió anteriormente.\n","\n","\n","\n","Para tener un mayor entendimiento del problema, se ubica el taxi en unas coordenadas especificas y frente a un estado deseado de la ubicacion del pasajero y su destino, se prueban diferentes ubicaciones para entender el problema"]},{"cell_type":"code","metadata":{"id":"g2PC6P0u-yxN","executionInfo":{"status":"aborted","timestamp":1681494301143,"user_tz":300,"elapsed":20,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["estado = taxi.encode(0, 1, 4, 1) # (ubicacion fila, ubicacion columna, ubicacion pasajero, destino del pasajero)\n","print(\"Estado:\", estado)\n","\n","taxi.s = estado\n","taxi.render()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QHOxsmJpA6e4"},"source":["**La tabla de recompensas**\n","\n","Para tener un mayor entendimiento del problema, se ubica el taxi en unas coordenadas especificas y frente a un estado deseado de la ubicacion del pasajero y su destino, se prueban diferentes ubicaciones para entender el problema\n","\n","Cuando el ambiente \"Taxi\" es creado, también hay una tabla de recompensas que se crea. Se puede pensar en esta como una matriz que tiene el número de estados como filas y el número de acciones como columnas:\n","\n","Dado que cada estado está en la matriz, podemos ver los valores de recompensa por defecto asignados a nuestra imagen.\n","\n","Para el analisis de esta problematica, se decide elegir un estado igual a 37 para ubicar el taxi, el pasajero y su destino"]},{"cell_type":"code","metadata":{"id":"Bhvt9k9z-5uH","executionInfo":{"status":"aborted","timestamp":1681494301144,"user_tz":300,"elapsed":21,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["taxi.P[37]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QeHwKdTi6Dc7"},"source":["Este diccionario es de la forma: \n","\n","```\n","{accion0: [(probabilidad, siguiente estado, recompensa, ¿listo?)], \n"," accion1: [(probabilidad, siguiente estado, recompensa, ¿listo?)], \n"," ... , \n"," accion5:[(probabilidad, siguiente estado, recompensa, ¿listo?)]}\n","```\n","\n","Anotaciones: \n","- 0 a 5 corresponde a las acciones (south, north, east, west, pickup, dropoff) que el taxi puede realizar en nuestro actual estado (en la imagen)\n","- En este ambiente, la probabilidad (```probabilidad```) de que algo pase siempre es $1.0$\n","- Siguiente estado (```siguiente estado```) es el estado al que pasaría si se toma la acción.\n","- Para este estado, todas las acciones de movimiento tienen una recompensa (```recompensa```) de $-1$, y las acciones de  pickup/dropoff de $-10$. Si estuvieramos en un estado en el destino del pasajero, la  acción de dropoff  aparecería una recompensa de $20$ para esta acción. \n","- ```listo``` es usado para decirno si el pasajero fue dejado en su destino exitosamente. Por cada pasajero dejado en su destino de manera exitosa se completa un episodio. \n","\n","Notemos que si el agente explora la acción 2 en este estado, va a \"chocar\" contra una pared, el algoritmo hace que sea imposible moverse a través de una pared, por lo que si se elije esta acción se irán acumulando penalizaciones de $-1$, lo que afectar a la **recompensa a largo plazo**\n","\n","\n","\n","Una vez observados los posibles movimientos, se procede a fijar esa posición como el estado de origen en donde incluso se tiene definido hacia donde se quiere llevar el pasajero y donde se debe recoger."]},{"cell_type":"markdown","metadata":{"id":"m0lxouS31aBc"},"source":["## Resolviendo el ambiente **SIN** aprendizaje reforzado\n","\n","Miremos que pasaría si se quiere resolver el problema a partir de la \"fuerza bruta\"\n","\n","Como tenemos la tabla de recompensas para cada estado (```P```), se puede poner a navegar al taxi solo con eso. \n","\n","Se crea un ciclo infinito que corra hasta que un pasajero sea dejado a su destino de manera correcta (esto es un **episodio**), o en otras palabras, que se obtenga la recompensa de $20$. El método ```taxi.action_space.sample()``` automaticamente selecciona una acción aleatoria del conjunto de posibles acciones. \n","\n","Miremos qué pasa: "]},{"cell_type":"code","metadata":{"id":"k1GvcrbM_NzM","executionInfo":{"status":"aborted","timestamp":1681494301145,"user_tz":300,"elapsed":22,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["taxi.s = 37  # se fija el estado deseado\n","\n","pasos = 0\n","castigos, recompensa = 0, 0\n","\n","frames = [] # for animation\n","\n","done = False\n","\n","while not done:\n","    accion = taxi.action_space.sample()\n","    estado, recompensa, done, info = taxi.step(accion)\n","\n","    if recompensa == -10:\n","        castigos += 1\n","    \n","    # Put each rendered frame into dict for animation\n","    frames.append({\n","        'frame': taxi.render(mode='ansi'),\n","        'estado': estado,\n","        'acción': accion,\n","        'recompensa': recompensa\n","        }\n","    )\n","\n","    pasos += 1\n","    \n","    \n","print(\"Pasos tomados: {}\".format(pasos))\n","print(\"Castigos incurridos: {}\".format(castigos))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ErGI0Qv66-WH"},"source":["Se observa que se necesitó mas de mil pasos para ir de la posición inicial a recoger el pasajero y dejarlo en su lugar de destino, de los cuales se cometio un buen numero de fallas. A continuación se visualiza cada uno de los movimientos realizados por el taxi (Estos datos pueden variar según la posición en la que se ubique el taxi cuando este se resetea la posición inicial)"]},{"cell_type":"code","metadata":{"id":"V6TsBZKI_TZ5","executionInfo":{"status":"aborted","timestamp":1681494301146,"user_tz":300,"elapsed":23,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["from IPython.display import clear_output\n","from time import sleep\n","\n","def print_frames(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'])\n","        print(f\"Pasos: {i + 1}\")\n","        print(f\"estado: {frame['estado']}\")\n","        print(f\"accion: {frame['acción']}\")\n","        print(f\"recompensa: {frame['recompensa']}\")\n","        sleep(.1)\n","        \n","print_frames(frames)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aoyAk00p77Pu"},"source":["### Implementando *Q-learning*\n","\n","A partir de este momento se crea el modelo de entrenamiento reforzado para que el sistema aprenda y sea capaz de cometer el menor numero de faltas y optimizar los pasos necesarios."]},{"cell_type":"code","metadata":{"id":"ol68WOsWB6p1","executionInfo":{"status":"aborted","timestamp":1681494301146,"user_tz":300,"elapsed":22,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["import numpy as np\n","q_table = np.zeros([taxi.observation_space.n, taxi.action_space.n])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJYhyQLb9gd2"},"source":["Ahora podemos crear un algoritmo de entrenamiento que actualice esta matriz-Q a medida que el agente explora el ambiente en varios episodios.\n","\n","El modelo consiste en definir los hiperparametros del proceso, es decir los valores de alpha (tasa de aprendizaje), gamma (factor de descuento), y epsilon (medida de mejora en el aprendizaje). Estos hiperparametros son de gran importancia dentro del modelo de aprendizaje.\n","\n","---\n","\n","En la primera parte de while not done, se decide si se quieren tomar acciones aleatorias (explorar) o si se quieren explotar los valores-Q ya aprendidos. Esto se hace utilizando el valor  ϵ  y comparandolo con un valor aleatorio random.uniform(0,1).\n","\n","El modelo de entrenamiento indica que si el numero aleatorio es menor al epsilon, entonces la accion es cualquier (explorar), de lo contrario se determina la proxima accion segun q usando los parametros alpha y gamma (explotar)\n","\n"]},{"cell_type":"code","metadata":{"id":"08LabOxBCIfV","executionInfo":{"status":"aborted","timestamp":1681494301147,"user_tz":300,"elapsed":23,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["%%time\n","import random\n","from IPython.display import clear_output\n","\n","# Hiperparametros\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# Para mostrar las metricas\n","Pasos_Totales = []\n","Fallas_totales = []\n","\n","for i in range(1, 100001):\n","    estado = taxi.reset()\n","\n","    pasos, castigos, recompensa, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            accion = taxi.action_space.sample() \n","        else:\n","            accion = np.argmax(q_table[estado]) \n","\n","        prox_estado, recompensa, done, info = taxi.step(accion) \n","        \n","        valor_anterior = q_table[estado, accion]\n","        max_proximo = np.max(q_table[prox_estado])\n","        \n","        nuevo_valor = (1 - alpha) * valor_anterior + alpha * (recompensa + gamma * max_proximo)\n","        q_table[estado, accion] = nuevo_valor\n","\n","        if recompensa == -10:\n","            castigos += 1\n","\n","        estado = prox_estado\n","        pasos += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Escenario: {i}\")\n","\n","print(\"Entrenamiento finalizado.\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WTfNFo5jAaoz"},"source":["Se visualiza el vector de entrenamiento para el estado deseado\n","\n","Ya tenemos la matriz-Q después de varios episodios, miremos los valores-Q para nuestra imágen. <br>\n","\n","Recordemos que el episodio era el número $37$. Y que: <br>\n","**Acciones**\n","- 0 = south\n","- 1 = north\n","- 2 = east\n","- 3 = west\n","- 4 = pickup\n","- 5 = dropoff\n"]},{"cell_type":"code","metadata":{"id":"8Ecl6dUsCgNd","executionInfo":{"status":"aborted","timestamp":1681494301148,"user_tz":300,"elapsed":24,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["q_table[37]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FOEF6TznAtaq"},"source":["Posicion 0 es la mejor\n","\n","\n","Se evalua de nuevo el taxi despues del entrenamiento reforzado"]},{"cell_type":"code","metadata":{"id":"jRkkXXCsCj01","executionInfo":{"status":"aborted","timestamp":1681494301152,"user_tz":300,"elapsed":28,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["Pasos_Totales, Fallas_totales = 0, 0\n","episodios = 100\n","\n","for _ in range(episodios):\n","    estado = taxi.reset()\n","    pasos, fallas, recompensa = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        accion = np.argmax(q_table[estado])\n","        estado, recompensa, done, info = taxi.step(accion)\n","\n","        if recompensa == -10:\n","            fallas += 1\n","\n","        pasos += 1\n","\n","    Fallas_totales += fallas\n","    Pasos_Totales += pasos\n","\n","print(f\"Resultados despues de {episodios} episodios:\")\n","print(f\"Promedio de pasos por episodio: {Pasos_Totales / episodios}\")\n","print(f\"Promedio fallas por episodio: {Fallas_totales / episodios}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pvZBqotNBibV"},"source":["Se observa que en 100 episodios no se cometio ni una falla por lo que el taxi quedó con un buen ajuste de entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"pSJKO5nhgUWe"},"source":["Se puede evaluar como cambia el modelo si se modifican los hiperparametros, para ver la sensibilidad que este tiene, primero se realiza con alpha que es la tasa de aprendizaje, cambiando esta de 0.1 a 0.4"]},{"cell_type":"code","metadata":{"id":"xNL05A3pi278","executionInfo":{"status":"aborted","timestamp":1681494301152,"user_tz":300,"elapsed":27,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["Pasos_Totales, Fallas_totales = 0, 0\n","episodios = 100\n","\n","for _ in range(episodios):\n","    estado = taxi.reset()\n","    pasos, fallas, recompensa = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        accion = np.argmax(q_table[estado])\n","        estado, recompensa, done, info = taxi.step(accion)\n","\n","        if recompensa == -10:\n","            fallas += 1\n","\n","        pasos += 1\n","\n","    Fallas_totales += fallas\n","    Pasos_Totales += pasos\n","\n","print(f\"Resultados despues de {episodios} episodios:\")\n","print(f\"Promedio de pasos por episodio: {Pasos_Totales / episodios}\")\n","print(f\"Promedio fallas por episodio: {Fallas_totales / episodios}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2agK46ui9Wq"},"source":["Cuando se observa el resultado, se encuentra que el promedio incrementa un poco, y tiempo de procesamiento es el mismo, por lo que no hay mucha sensibilidad de alpha en el modelo de aprendizaje, luego se retorna alpha a 0.1 y se cambia gamma de 0.6 a 0.4 la tasa de descuento"]},{"cell_type":"code","source":[],"metadata":{"id":"epnctCY-5Fvx","executionInfo":{"status":"aborted","timestamp":1681494301153,"user_tz":300,"elapsed":28,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"huvzACyBjSsG","executionInfo":{"status":"aborted","timestamp":1681494301153,"user_tz":300,"elapsed":28,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["%%time\n","\n","q_table = np.zeros([taxi.observation_space.n, taxi.action_space.n])\n","\n","\n","import random\n","from IPython.display import clear_output\n","\n","# Hiperparametros\n","alpha = 0.1\n","gamma = 0.4\n","epsilon = 0.1\n","\n","# Para mostrar las metricas\n","Pasos_Totales = []\n","Fallas_totales = []\n","\n","for i in range(1, 100001):\n","    estado = taxi.reset()\n","\n","    pasos, castigos, recompensa, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            accion = taxi.action_space.sample() \n","        else:\n","            accion = np.argmax(q_table[estado]) \n","\n","        prox_estado, recompensa, done, info = taxi.step(accion) \n","        \n","        valor_anterior = q_table[estado, accion]\n","        max_proximo = np.max(q_table[prox_estado])\n","        \n","        nuevo_valor = (1 - alpha) * valor_anterior + alpha * (recompensa + gamma * max_proximo)\n","        q_table[estado, accion] = nuevo_valor\n","\n","        if recompensa == -10:\n","            castigos += 1\n","\n","        estado = prox_estado\n","        pasos += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Escenario: {i}\")\n","\n","print(\"Entrenamiento finalizado.\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnQmXciSjZG3","executionInfo":{"status":"aborted","timestamp":1681494301154,"user_tz":300,"elapsed":29,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["Pasos_Totales, Fallas_totales = 0, 0\n","episodios = 100\n","\n","for _ in range(episodios):\n","    estado = taxi.reset()\n","    pasos, fallas, recompensa = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        accion = np.argmax(q_table[estado])\n","        estado, recompensa, done, info = taxi.step(accion)\n","\n","        if recompensa == -10:\n","            fallas += 1\n","\n","        pasos += 1\n","\n","    Fallas_totales += fallas\n","    Pasos_Totales += pasos\n","\n","print(f\"Resultados despues de {episodios} episodios:\")\n","print(f\"Promedio de pasos por episodio: {Pasos_Totales / episodios}\")\n","print(f\"Promedio fallas por episodio: {Fallas_totales / episodios}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vmuHdlSlkDWd"},"source":["Cambiando gama, aunque requiere un poco mas de tiempo, se requieren menos pasos por episodio para evaluar el modelo y aun asi no ocurre fallas, por ultimo  se cambia epsilon a 0.5 retornando gamma a su valor de 0.6"]},{"cell_type":"code","metadata":{"id":"DPzCGKkokNdW","executionInfo":{"status":"aborted","timestamp":1681494301154,"user_tz":300,"elapsed":29,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["%%time\n","\n","q_table = np.zeros([taxi.observation_space.n, taxi.action_space.n])\n","\n","\n","import random\n","from IPython.display import clear_output\n","\n","# Hyperparametros\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.5\n","\n","# Para mostrar las metricas\n","Pasos_Totales = []\n","Fallas_totales = []\n","\n","for i in range(1, 100001):\n","    estado = taxi.reset()\n","\n","    pasos, castigos, recompensa, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            accion = taxi.action_space.sample() \n","        else:\n","            accion = np.argmax(q_table[estado]) \n","\n","        prox_estado, recompensa, done, info = taxi.step(accion) \n","        \n","        valor_anterior = q_table[estado, accion]\n","        max_proximo = np.max(q_table[prox_estado])\n","        \n","        nuevo_valor = (1 - alpha) * valor_anterior + alpha * (recompensa + gamma * max_proximo)\n","        q_table[estado, accion] = nuevo_valor\n","\n","        if recompensa == -10:\n","            castigos += 1\n","\n","        estado = prox_estado\n","        pasos += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Escenario: {i}\")\n","\n","print(\"Entrenamiento finalizado.\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0nNdMZqkcnE","executionInfo":{"status":"aborted","timestamp":1681494301155,"user_tz":300,"elapsed":30,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"}}},"source":["Pasos_Totales, Fallas_totales = 0, 0\n","episodios = 100\n","\n","for _ in range(episodios):\n","    estado = taxi.reset()\n","    pasos, fallas, recompensa = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        accion = np.argmax(q_table[estado])\n","        estado, recompensa, done, info = taxi.step(accion)\n","\n","        if recompensa == -10:\n","            fallas += 1\n","\n","        pasos += 1\n","\n","    Fallas_totales += fallas\n","    Pasos_Totales += pasos\n","\n","print(f\"Resultados despues de {episodios} episodios:\")\n","print(f\"Promedio de pasos por episodio: {Pasos_Totales / episodios}\")\n","print(f\"Promedio fallas por episodio: {Fallas_totales / episodios}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hbDZ5M_k5jr"},"source":["## Tareas\n","\n","1) Que pasa cuando se modifica epsilon?, es mas rapido?? menos exigente? se requieren mas paso?? hagan una tabla de valores de epsilo para ese analisis\n","\n","2) Cuales son los 3 hiperparametros en donde sea optimo el tiempo de maquina y el numero de pasos requeridos para encontrar la solución.\n","\n","3) Que pasa si se cambia el estado inicial?"]}]}