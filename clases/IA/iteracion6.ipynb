{"cells":[{"cell_type":"markdown","metadata":{"id":"nDB0AnIhW_UI"},"source":["#Traductor para Gregos"]},{"cell_type":"markdown","metadata":{"id":"ann_8m4ifDkD"},"source":["importamos las librerias, haremos la red neuronal con tensorflow y keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvUc4F1gWwMj"},"outputs":[],"source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n"]},{"cell_type":"markdown","metadata":{"id":"PF_xf4eWW-7L"},"source":["Descargamos un dataset que proporciona tensorflow de español y ingles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOCCtMlqXAtK"},"outputs":[],"source":["text_file = keras.utils.get_file(\n","    fname=\"spa-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","    extract=True,\n",")\n","text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""]},{"cell_type":"markdown","metadata":{"id":"Q3RoPBShfdh1"},"source":["Leemos el dataset y Creamos pares de cada frase con su significado en inlges y español.\n","\n","se usa frases para que el modelo entienda el contexto en el que se usa, no es lo mismo decir\n","\n","\n","\n","```\n","No, te pertenece\n","```\n","a\n","\n","```\n","No te pertenece\n","```\n","\n","ambas palabaras tienen significados opuestos que depende mas que de las palabras que se usen , que contexto tiene toda la frase\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683308600950,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"},"user_tz":300},"id":"JxF-9insXYxm","outputId":"2f9039bc-315a-400a-e9f0-9e66c6df5510"},"outputs":[{"name":"stdout","output_type":"stream","text":["('As soon as I sat down, I fell asleep.', '[start] En cuanto tomé asiento, caí dormido. [end]')\n","('We regret his death.', '[start] Lamentamos su fallecimiento. [end]')\n","('Two plus two makes four.', '[start] Dos más dos es cuatro. [end]')\n","('Think of your family.', '[start] ¡Piense en su familia! [end]')\n","('What did you buy for your boyfriend?', '[start] ¿Qué le compraste a tu amigo? [end]')\n"]}],"source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    eng, spa = line.split(\"\\t\")\n","    spa = \"[start] \" + spa + \" [end]\"\n","    text_pairs.append((eng, spa))\n","for _ in range(5):\n","    print(random.choice(text_pairs))\n"]},{"cell_type":"markdown","metadata":{"id":"9y3oibFug1nj"},"source":["Como dije entregas anteriores una buena practica es separar los datos en 2 conjuntos , uno de entrenamiento ,validacion y otro de prueba para mejorar el model, bueno aqui estoy haciendo eso\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1683308601183,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"},"user_tz":300},"id":"kIcgzj9ufay1","outputId":"909755d8-c2d0-4bbd-91b4-0de00dc99f51"},"outputs":[{"name":"stdout","output_type":"stream","text":["118964 total pairs\n","83276 training pairs\n","17844 validation pairs\n","17844 test pairs\n"]}],"source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"]},{"cell_type":"markdown","metadata":{"id":"Da4HQT5MjJ_x"},"source":["definimos nuestros hiperparametros"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwH-9EUDjKzN"},"outputs":[],"source":["vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64"]},{"cell_type":"markdown","metadata":{"id":"dn337H6hh-2V"},"source":["Hacemos una funcion que nos limpiara los datos cuando los usemos para vectorizarlos con la funcion \n","\n","```\n","custom_standardization()\n","```\n","esta funcion convierte todo a minusculas y elimina algunos signos de puntuacion\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCbuyhAAh9b4"},"outputs":[],"source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")"]},{"cell_type":"markdown","metadata":{"id":"pMQZJJXJju8J"},"source":["Vectorizamos las palabras, este proceso consiste en dividir el string en tokens, construye un vocabulario y como en las redes neuronales no se pueden usar strings, construye un vocabulario como una lista y usa el indice de la palabra para entrenar la red, luego hace una matriz de dispercion para ver que tan parecidas son las palabras\n","\n","![link text](https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png?hl=es-419)\n","\n","matriz de dispercion de ejemplo de ingles a portuges, se observa que mientras mas parecidas las palabras su color en la matriz es mas brillante "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGS20vSUj7dy"},"outputs":[],"source":["eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","spa_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)\n","def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"]},{"cell_type":"markdown","metadata":{"id":"CnSBQPVZpXA9"},"source":["Hacemos nuestra implementacion de los transoformes, los transoformes son un arquitectura de redes neuronales,muy potentes por la \"atencion\", es la pieza principal de chatgpt.\n","\n","imagen de la nn que represnta los transformers\n","![trasnformer](https://www.tensorflow.org/images/tutorials/transformer/transformer.png)\n","\n","\n","dividimos el transformer en 2, uno para codificar y otro para decodificar, en otros codigos usan solo un transoforme que codifique y decodifique, yo lo prefiero en 2 por orden en el codigo\n","\n","¿que son los multihead attention?\n","\n","son capas de las redes de la red que examinan la relacion entre las palabras\n","\n","¿que son los softmax?\n","\n","otra capa de la red que crea probabilidades, esto es importante usarlo en claisificacion para esocjer las opciones con mayor probabilidad\n","\n","¿que son los linear?\n","\n","son capas que hacen un proceso wx+b a todos los elementos de un vector\n"]},{"cell_type":"markdown","metadata":{"id":"TlLeSGjMAZ5i"},"source":["## Transoformer Codificador "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZM2PLyGrAe1S"},"outputs":[],"source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n"]},{"cell_type":"markdown","metadata":{"id":"32qNpghiAfbC"},"source":["## decodificador del Transformer  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tf2je402_lYN"},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)"]},{"cell_type":"markdown","metadata":{"id":"PuSqpvUnAz2K"},"source":["Terminamos de ensablar el model, antes habiamos hecho los transoformers , ya juntamos el transoformer al resto del la NN.\n","\n","al final llamamos la funcion donde hacemos el ensamble y le pasamos los parametros de capas de entrada y salida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpCHuA9kXfVR"},"outputs":[],"source":["def create_model(embed_dim ,latent_dim,num_heads,vocab_size,sequence_length):\n","  encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","  x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","  encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","  encoder = keras.Model(encoder_inputs, encoder_outputs)\n","  decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","  encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","  x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","  x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","  x = layers.Dropout(0.5)(x)\n","  decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","  decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","  decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","  transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n","  )\n","  return transformer\n","\n","embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","transormer=create_model(embed_dim ,latent_dim,num_heads,vocab_size,sequence_length)"]},{"cell_type":"markdown","metadata":{"id":"bX4MnXFICZ5f"},"source":["Ya es hora de entrenar el modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"D5WiQAkzeiAA","outputId":"bea00435-cbbd-424e-a78d-76f628cea995"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," positional_embedding_2 (Positi  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n"," onalEmbedding)                                                                                   \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," transformer_encoder_1 (Transfo  (None, None, 256)   3155456     ['positional_embedding_2[0][0]'] \n"," rmerEncoder)                                                                                     \n","                                                                                                  \n"," model_3 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n","                                                                  'transformer_encoder_1[0][0]']  \n","                                                                                                  \n","==================================================================================================\n","Total params: 19,960,216\n","Trainable params: 19,960,216\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/10\n","1302/1302 [==============================] - 4247s 3s/step - loss: 2.8733 - accuracy: 0.5471 - val_loss: 2.5397 - val_accuracy: 0.5877\n","Epoch 2/10\n","1302/1302 [==============================] - 4086s 3s/step - loss: 2.5648 - accuracy: 0.5919 - val_loss: 2.4021 - val_accuracy: 0.6144\n","Epoch 3/10\n","   1/1302 [..............................] - ETA: 57:45 - loss: 2.4249 - accuracy: 0.6087"]}],"source":["epochs = 10#30  epochs es para mejor calidad\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n","spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range():\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decode_sequence(input_sentence)\n","print(translated)"]},{"cell_type":"markdown","metadata":{"id":"ohljok1xDcfi"},"source":["Finalmente ya tenemos el traductor\n","\n","ingresa una palabra en ingles , para hacer lo de español a ingles se hace el proceso al revez.\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7424,"status":"ok","timestamp":1683313234125,"user":{"displayName":"jero θηζζβ","userId":"02987265110197884933"},"user_tz":300},"id":"cwJlDM4owYbl","outputId":"1391ffe0-8c2f-4819-cb17-1cff6c69f89c"},"outputs":[{"name":"stdout","output_type":"stream","text":["how are you?\n","how are you?\n","[start] cómo son usted [end]\n"]}],"source":["txt=input()\n","print(txt)\n","translated = decode_sequence(txt)\n","print(translated)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNELl+7myjTFugTmqhGxo8g"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}