1. Install PySpark on your local machine or on a cloud-based platform like Databricks or AWS EMR.

Install the necessary Python libraries to connect to MongoDB, SQL, and Neo4j. For example, you can use pymongo to connect to MongoDB, PyMySQL or SQLAlchemy to connect to SQL databases, and py2neo to connect to Neo4j.

Define the data schema for each data source. This step involves understanding the data structure of each data source and creating a schema that can be used in PySpark. For SQL databases, you can use the DataFrame API to infer the schema from the database. For MongoDB and Neo4j, you may need to manually define the schema.

Load the data from each data source into PySpark DataFrames. You can use the libraries mentioned in step 2 to read data from MongoDB, SQL, and Neo4j, and create PySpark DataFrames.

Join the DataFrames based on a common key or attribute. You can use the DataFrame API to join DataFrames and create a single DataFrame that contains data from all three data sources.

Perform data analysis using PySpark. You can use PySpark's powerful distributed computing capabilities to perform data analysis on the joined DataFrame.

Write the results to a target data store. You can use PySpark's DataFrame API to write the results to a target data store, such as another SQL database or a file system.

Overall, connecting MongoDB, SQL, and Neo4j with PySpark can be a complex task, but it can also provide significant benefits in terms of data analysis and processing capabilities.